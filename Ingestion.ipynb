{"cells":[{"cell_type":"code","source":["#list of the stocks\n\nInitial_List = [\"AAPl\", \"FE\", \"TSLA\", \"PEP\", \"MNOV\", \"SHOP\", \"IRBT\", \"SIX\", \"NNVC\", \"HBSC\", \"NOC\", \"RTN\", \"PBR\", \"BBD\", \"IBM\", \"SAP\", \"ACN\", \"TXN\", \"NKE\", \"DIS\", \"JPM\", \"AME\", \"MSG\", \"SBUX\", \"CNNE\", \"GIS\", \"PG\", \"KO\", \"AMRS\", \"HEI\", \"MSFT\", \"AMD\", \"AMZN\", \"FB\", \"NFLX\", \"ORCL\", \"SPCB\", \"ELY\", \"CVX\", \"RHNO\", \"CECO\"]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["#Getting the historical Data by symbol\ndef GetHistoricalData(symbol):\n  try:\n    start = datetime.datetime(2016, 1, 1)\n    end = datetime.datetime.today().date()\n    result = get_historical_data(symbol, start, end, token=IEX_token)\n    WriteToDataLake(result, symbol +\"/Hist/\")\n  except:\n    pass\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["#Writing to DataLake\n\ndef WriteToDataLake(pdf, folderpath):\n#   df = pandas_to_spark(pdf)\n  df = spark.createDataFrame(pdf)\n  df.repartition(1).write.mode(\"append\").format('csv').options(header='true', delimiter=',', inferSchema='true').save(\"abfss://iex@hhaggandl2.dfs.core.windows.net/MMAI_Finance2/\"+folderpath)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["import pandas as pd\nimport quandl\nimport datetime\n\nfrom iexfinance.stocks import Stock\nfrom iexfinance.altdata import get_social_sentiment\nfrom iexfinance.stocks import get_historical_data\nfrom iexfinance.refdata import get_symbols\nfrom iexfinance.stocks import get_historical_intraday\n\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import SparkSession\nfrom pyspark import SparkConf\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.types import *\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import from_json, to_json\nfrom pyspark.sql.types import DateType, StringType, StructType\nimport json"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["#Initializaing the main services\n\n#region IEX Configuration\n\nIEX_URL = \"abfss://iex@hhaggandl2.dfs.core.windows.net/MMAI/\"\n\n#region Quandl password\nquandl.ApiConfig.api_key = \"7tPpbtseQHTGtt4RjPci\"\n#endregion\n\n#region IEX password\nIEX_token = \"sk_37e05de0f447447bba587440923da37a\"\n#endregion\n\n#endregion\n\n#region Datalake Configuration\nspark.conf.set(\"fs.azure.account.key.hhaggandl2.dfs.core.windows.net\", \"EjU3j412niQaDfXUG5XDVaybkszqiPCcO9+x6fXTRNUnhpgV5Dsu6Fg33uItZ/bLXXHIYfQrDBmC7Bu/ADQJ9A==\")\nspark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\ndbutils.fs.ls(\"abfss://iex@hhaggandl2.dfs.core.windows.net/\")\nspark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")\n\n#endregion\n\n#region Symbols\n\ndef GetSymbols():\n    return get_symbols(output_format='pandas', token=IEX_token)\n\n# #endregion\n\nAll_Symbols = GetSymbols()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["import io\nimport pandas\nimport requests\nimport datetime\n\nbase = \"https://www.quandl.com/api/v3/datasets/WIKI/\"\napi = \"7tPpbtseQHTGtt4RjPci\"\n\ndef fun(stock):\n  resp = requests.get(base+stock+'/data.csv?api_key='+ api)\n  s = resp.content\n  df = pd.read_csv(io.StringIO(s.decode('utf-8')))\n  WriteToDataLake(df, stock)\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["for stock_symbol in Initial_List:\n  fun(stock_symbol)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["for stock_symbol in All_Symbols.symbol:\n  fun(stock_symbol)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-4179646198054075&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansigreen\">for</span> stock_symbol <span class=\"ansigreen\">in</span> All_Symbols<span class=\"ansiyellow\">.</span>symbol<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\">   </span>fun<span class=\"ansiyellow\">(</span>stock_symbol<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">&lt;command-873924226432057&gt;</span> in <span class=\"ansicyan\">fun</span><span class=\"ansiblue\">(stock)</span>\n<span class=\"ansigreen\">     11</span>   s <span class=\"ansiyellow\">=</span> resp<span class=\"ansiyellow\">.</span>content<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     12</span>   df <span class=\"ansiyellow\">=</span> pd<span class=\"ansiyellow\">.</span>read_csv<span class=\"ansiyellow\">(</span>io<span class=\"ansiyellow\">.</span>StringIO<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>decode<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;utf-8&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 13</span><span class=\"ansiyellow\">   </span>WriteToDataLake<span class=\"ansiyellow\">(</span>df<span class=\"ansiyellow\">,</span> stock<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     14</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">&lt;command-3841365661557872&gt;</span> in <span class=\"ansicyan\">WriteToDataLake</span><span class=\"ansiblue\">(pdf, folderpath)</span>\n<span class=\"ansigreen\">      3</span> <span class=\"ansigreen\">def</span> WriteToDataLake<span class=\"ansiyellow\">(</span>pdf<span class=\"ansiyellow\">,</span> folderpath<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> <span class=\"ansired\">#   df = pandas_to_spark(pdf)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 5</span><span class=\"ansiyellow\">   </span>df <span class=\"ansiyellow\">=</span> spark<span class=\"ansiyellow\">.</span>createDataFrame<span class=\"ansiyellow\">(</span>pdf<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      6</span>   df<span class=\"ansiyellow\">.</span>repartition<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>write<span class=\"ansiyellow\">.</span>mode<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;append&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>format<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;csv&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>options<span class=\"ansiyellow\">(</span>header<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&apos;true&apos;</span><span class=\"ansiyellow\">,</span> delimiter<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&apos;,&apos;</span><span class=\"ansiyellow\">,</span> inferSchema<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&apos;true&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>save<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;abfss://iex@hhaggandl2.dfs.core.windows.net/MMAI_Finance2/&quot;</span><span class=\"ansiyellow\">+</span>folderpath<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">createDataFrame</span><span class=\"ansiblue\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansigreen\">    815</span>                 rdd<span class=\"ansiyellow\">,</span> schema <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_createFromRDD<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span>prepare<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">,</span> samplingRatio<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    816</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 817</span><span class=\"ansiyellow\">                 </span>rdd<span class=\"ansiyellow\">,</span> schema <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_createFromLocal<span class=\"ansiyellow\">(</span>map<span class=\"ansiyellow\">(</span>prepare<span class=\"ansiyellow\">,</span> data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    818</span>             jrdd <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>SerDeUtil<span class=\"ansiyellow\">.</span>toJavaArray<span class=\"ansiyellow\">(</span>rdd<span class=\"ansiyellow\">.</span>_to_java_object_rdd<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    819</span>             jdf <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jsparkSession<span class=\"ansiyellow\">.</span>applySchemaToPythonRDD<span class=\"ansiyellow\">(</span>jrdd<span class=\"ansiyellow\">.</span>rdd<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">.</span>json<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">_createFromLocal</span><span class=\"ansiblue\">(self, data, schema)</span>\n<span class=\"ansigreen\">    440</span>         write temp files<span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    441</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 442</span><span class=\"ansiyellow\">         </span>data<span class=\"ansiyellow\">,</span> schema <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_wrap_data_schema<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    443</span>         <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">.</span>parallelize<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    444</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">_wrap_data_schema</span><span class=\"ansiblue\">(self, data, schema)</span>\n<span class=\"ansigreen\">    419</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    420</span>         <span class=\"ansigreen\">if</span> schema <span class=\"ansigreen\">is</span> <span class=\"ansigreen\">None</span> <span class=\"ansigreen\">or</span> isinstance<span class=\"ansiyellow\">(</span>schema<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span>list<span class=\"ansiyellow\">,</span> tuple<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 421</span><span class=\"ansiyellow\">             </span>struct <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_inferSchemaFromList<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">,</span> names<span class=\"ansiyellow\">=</span>schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    422</span>             converter <span class=\"ansiyellow\">=</span> _create_converter<span class=\"ansiyellow\">(</span>struct<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    423</span>             data <span class=\"ansiyellow\">=</span> map<span class=\"ansiyellow\">(</span>converter<span class=\"ansiyellow\">,</span> data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">_inferSchemaFromList</span><span class=\"ansiblue\">(self, data, names)</span>\n<span class=\"ansigreen\">    350</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    351</span>         <span class=\"ansigreen\">if</span> <span class=\"ansigreen\">not</span> data<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 352</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">raise</span> ValueError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;can not infer schema from empty dataset&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    353</span>         first <span class=\"ansiyellow\">=</span> data<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    354</span>         <span class=\"ansigreen\">if</span> type<span class=\"ansiyellow\">(</span>first<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">is</span> dict<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">ValueError</span>: can not infer schema from empty dataset</div>"]}}],"execution_count":8}],"metadata":{"name":"Ingestion","notebookId":3841365661557869},"nbformat":4,"nbformat_minor":0}
