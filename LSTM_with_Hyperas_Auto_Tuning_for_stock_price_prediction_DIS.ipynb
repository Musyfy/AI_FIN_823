{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "LSTM with Hyperas - Auto Tuning for stock price prediction - DIS.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Musyfy/AI_FIN_823/blob/master/LSTM_with_Hyperas_Auto_Tuning_for_stock_price_prediction_DIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcra-dw0yh-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import quandl\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import dask.dataframe as dd\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers import Dense, Conv1D, MaxPool2D, Flatten, Dropout, CuDNNLSTM, CuDNNGRU, Conv2D, MaxPooling2D\n",
        "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "from keras.optimizers import Adam, SGD, Nadam\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from time import time\n",
        "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
        "#from keras.utils.training_utils import multi_gpu_model\n",
        "from tensorflow.python.client import device_lib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "#from livelossplot import PlotLossesKeras\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.python.client import device_lib\n",
        "from keras import backend as K\n",
        "from keras import metrics\n",
        "from keras.utils import np_utils\n",
        "import keras as ks\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "import pandas_datareader.data as pd_reader\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u00R59d-yvHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pip install quandl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyI_2eRXy03E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pip install hyperas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsHP0KzBy7Kk",
        "colab_type": "code",
        "outputId": "06d74be1-2cae-49c4-9ebd-007978443c23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%ls /gdrive"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "\u001b[0m\u001b[01;34m'My Drive'\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx9aBM9hzH8m",
        "colab_type": "code",
        "outputId": "ac2a71f4-2311-4925-b7c4-0dd471263f3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!ls '/gdrive/My Drive/Colab Notebooks'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'LSTM with Hyperas - Auto Tuning for stock price prediction - AT&T.ipynb'\n",
            "'LSTM with Hyperas - Auto Tuning for stock price prediction - BAC.ipynb'\n",
            "'LSTM with Hyperas - Auto Tuning for stock price prediction - BACV2.ipynb'\n",
            "'LSTM with Hyperas - Auto Tuning for stock price prediction-Copy1.ipynb'\n",
            "'LSTM with Hyperas - Auto Tuning for stock price prediction - DIS.ipynb'\n",
            "'LSTM with Hyperas - Auto Tuning for stock price prediction - MA.ipynb'\n",
            "'LSTM with Hyperas - Auto Tuning for stock price prediction - PG.ipynb'\n",
            "'LSTM with Hyperas - Auto Tuning for stock price prediction - UNH.ipynb'\n",
            "'LSTM with Hyperas - Auto Tuning for stock price prediction - Visa.ipynb'\n",
            "'LSTM with Hyperas - Auto Tuning for stock price prediction - XOM.ipynb'\n",
            " Untitled0.ipynb\n",
            " Untitled1.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9i3VeJhAyh-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHAwuRIWyh-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#conda install -c delichon pandas_datareader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfSV6biiyh-W",
        "colab_type": "code",
        "outputId": "e5170104-0c03-4954-e9f0-0b8e8ebe0e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from platform import python_version\n",
        "print(python_version())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Eaf1P3F6yh-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pip install hyperas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN7WLcr1yh-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quandl.ApiConfig.api_key = \"uRMo697HgMj91ZZZa2_v\"\n",
        "df = quandl.get_table('WIKI/PRICES', qopts = { 'columns': ['ticker', 'date', 'adj_close','open','high','low','close'] }, ticker = ['DIS'], date = { 'gte': '2009-01-01', 'lte': '2019-12-19' })\n",
        "df = pd_reader.DataReader('DIS','yahoo')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPPAMfbtyh-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from hyperas.distributions import uniform\n",
        "\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from keras.datasets import mnist\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA3QF3AFyh-g",
        "colab_type": "text"
      },
      "source": [
        "### Make sure to edit your stock ticker in def data()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnPMfRrTyh-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data():\n",
        "    ###maka sure to change the stock ticker here \n",
        "    quandl.ApiConfig.api_key = \"uRMo697HgMj91ZZZa2_v\"\n",
        "    df = quandl.get_table('WIKI/PRICES', qopts = { 'columns': ['ticker', 'date', 'adj_close','open','high','low','close'] }, ticker = ['DIS'], date = { 'gte': '2009-01-01', 'lte': '2019-12-19' })\n",
        "    df = pd_reader.DataReader('DIS','yahoo')\n",
        "    df = df[['Adj Close','High','Open','Close','Low']] \n",
        "    df = df.dropna()\n",
        "    \n",
        "    factor_ratio = 0.7\n",
        "    df1 = df.iloc[:round(len(df)*factor_ratio)]\n",
        "    df2 = df[round(len(df)*factor_ratio):]\n",
        "    \n",
        "    training_set = df1.iloc[:,0:].values\n",
        "    sc = MinMaxScaler(feature_range = (0, 1))\n",
        "    training_set_scaled = sc.fit_transform(training_set)\n",
        "    \n",
        "    #train set\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for i in range(45, len(training_set_scaled)):\n",
        "        X_train.append(training_set_scaled[i-45:i])\n",
        "        y_train.append(training_set_scaled[i,0])\n",
        "\n",
        "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 5))\n",
        "    \n",
        "    #test set\n",
        "    y_test = df2.iloc[:,0:1].values\n",
        "\n",
        "    dataset_total = df.iloc[:,0:]\n",
        "    inputs = dataset_total[len(dataset_total) - len(df2) -45:].values\n",
        "    inputs = sc.fit_transform(inputs)\n",
        "\n",
        "    X_test = []\n",
        "    for i in range(45, len(inputs)):\n",
        "        X_test.append(inputs[i-45:i])\n",
        "    \n",
        "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 5))\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE0Zay3syh-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train, X_test, y_test = data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCBEfWOIyh-k",
        "colab_type": "text"
      },
      "source": [
        "### Things to consider \n",
        "1. create_model function: Change epochs to a lower number in the beginning\n",
        "2. create_model function: Start with lower number of layers - 4 was the best for me \n",
        "3. create_model function: The number of hidden layers is still to be tuned manually \n",
        "4. create_model function: if you are to add more layers. Make sure that the last layer is always set to (return_sequence=False) \n",
        "5. create_model function: you can add more parameters to tune: such as activation function, add more optimizers, or change dropout rate to a range between 0 & 1. \n",
        "6. if __name__ == '__main__' function: Make sure notebook_name matches the name of your notebook, otherwise it doesn't run. not sure how it would work on pycharm (you can try to remove this parameter) \n",
        "7. if __name__ == '__main__' function: you can change the number of max_eval to a lower number if you want this to run faster. This parameter is similar to a random search, and  will give you different models based on the max_eval number chosen. 3 or 4 should be okay with what I have seen so far. \n",
        "\n",
        "**This will take time if you are running it on your local machine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pweGTr00yh-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(X_train, y_train, X_test, y_test): \n",
        "    \"\"\"\n",
        "    Model providing function:\n",
        "\n",
        "    Create Keras model with double curly brackets dropped-in as needed.\n",
        "    Return value has to be a valid python dictionary with two customary keys:\n",
        "        - loss: Specify a numeric evaluation metric to be minimized\n",
        "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
        "    The last one is optional, though recommended, namely:\n",
        "        - model: specify the model just created so that we can later use it again.\n",
        "    \"\"\"\n",
        "      \n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units={{choice([100, 200, 300])}}, input_shape=(45,5), return_sequences= True))\n",
        "    model.add(Dropout(rate={{uniform(0.2, 0.5)}}))\n",
        "    \n",
        "    model.add(LSTM(units={{choice([100, 200, 300])}}, return_sequences= True))\n",
        "    model.add(Dropout(rate={{uniform(0.2, 0.5)}}))\n",
        "    \n",
        "    model.add(LSTM(units={{choice([100, 200, 300])}}, return_sequences= True))\n",
        "    model.add(Dropout(rate={{uniform(0.2, 0.5)}}))\n",
        "    \n",
        "    model.add(LSTM(units={{choice([100, 200, 300])}}, return_sequences= False))\n",
        "    model.add(Dropout(rate={{uniform(0.2, 0.5)}}))\n",
        "    \n",
        "    #model.add(LSTM(units={{choice([100, 200, 300])}}, return_sequences= False))\n",
        "    #model.add(Dropout(rate={{uniform(0.2, 0.5)}}))\n",
        "    \n",
        "    #model.add(LSTM(units={{choice([100, 200, 300])}}, return_sequences= False))\n",
        "    #model.add(Dropout(rate={{uniform(0.2, 0.5)}}))\n",
        "    \n",
        "    #model.add(LSTM(units={{choice([100, 200, 300])}}))\n",
        "    #model.add(Dropout(rate={{uniform(0.2, 0.5)}}))\n",
        "    \n",
        "    model.add(Dense(25))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    model.compile(loss='mean_squared_error',\n",
        "                  optimizer={{choice(['adam', 'sgd'])}})\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    result = model.fit(X_train, y_train,\n",
        "              batch_size={{choice([100,50,80,200])}},\n",
        "              epochs=100, verbose=2)\n",
        "    #get the highest validation accuracy of the training epochs\n",
        "    validation_error = np.amin(result.history['loss']) \n",
        "    print('Best validation error of epoch:', validation_error)\n",
        "    return {'loss': -validation_error, 'status': STATUS_OK, 'model': model}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8WEn-d5yh-m",
        "colab_type": "code",
        "outputId": "ea14f431-a0ba-454b-a183-2aca0f9eb89a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    best_run, best_model = optim.minimize(model=create_model,\n",
        "                                    data=data,\n",
        "                                    algo=tpe.suggest,\n",
        "                                    max_evals=4,\n",
        "                                    trials=Trials(),\n",
        "                                    notebook_name= os.path.join('..','gdrive','My Drive','Colab Notebooks','LSTM with Hyperas - Auto Tuning for stock price prediction - DIS'))\n",
        "    X_train, Y_train, X_test, Y_test = data()\n",
        "    print(\"Evalutation of best performing model:\")\n",
        "    print(best_model)\n",
        "    print(\"Best performing model chosen hyper-parameters:\")\n",
        "    print(best_run)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> Imports:\n",
            "#coding=utf-8\n",
            "\n",
            "from __future__ import print_function\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pandas as pd\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import quandl\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import matplotlib.pyplot as plt\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from mpl_toolkits.mplot3d import Axes3D\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import seaborn as sns\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tqdm import tqdm\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import dask.dataframe as dd\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import tensorflow as tf\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import sys\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers.recurrent import LSTM\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Dense, Conv1D, MaxPool2D, Flatten, Dropout, CuDNNLSTM, CuDNNGRU, Conv2D, MaxPooling2D\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.optimizers import Adam, SGD, Nadam\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers.normalization import BatchNormalization\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from time import time\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.python.client import device_lib\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.preprocessing import StandardScaler\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.model_selection import train_test_split\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras import backend as K\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperopt import Trials, STATUS_OK, tpe\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.preprocessing import MinMaxScaler\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.model_selection import train_test_split\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.python.client import device_lib\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras import backend as K\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras import metrics\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.utils import np_utils\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import keras as ks\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pickle\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import os\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pandas_datareader.data as pd_reader\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import warnings\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from google.colab import drive\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from platform import python_version\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import uniform\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperopt import Trials, STATUS_OK, tpe\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.datasets import mnist\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers.core import Dense, Dropout, Activation\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.utils import np_utils\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pickle\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from google.colab import files\n",
            "except:\n",
            "    pass\n",
            "\n",
            ">>> Hyperas search space:\n",
            "\n",
            "def get_space():\n",
            "    return {\n",
            "        'units': hp.choice('units', [100, 200, 300]),\n",
            "        'rate': hp.uniform('rate', 0.2, 0.5),\n",
            "        'units_1': hp.choice('units_1', [100, 200, 300]),\n",
            "        'rate_1': hp.uniform('rate_1', 0.2, 0.5),\n",
            "        'units_2': hp.choice('units_2', [100, 200, 300]),\n",
            "        'rate_2': hp.uniform('rate_2', 0.2, 0.5),\n",
            "        'units_3': hp.choice('units_3', [100, 200, 300]),\n",
            "        'rate_3': hp.uniform('rate_3', 0.2, 0.5),\n",
            "        'units_4': hp.choice('units_4', [100, 200, 300]),\n",
            "        'rate_4': hp.uniform('rate_4', 0.2, 0.5),\n",
            "        'units_5': hp.choice('units_5', [100, 200, 300]),\n",
            "        'rate_5': hp.uniform('rate_5', 0.2, 0.5),\n",
            "        'units_6': hp.choice('units_6', [100, 200, 300]),\n",
            "        'rate_6': hp.uniform('rate_6', 0.2, 0.5),\n",
            "        'optimizer': hp.choice('optimizer', ['adam', 'sgd']),\n",
            "        'batch_size': hp.choice('batch_size', [100,50,80,200]),\n",
            "    }\n",
            "\n",
            ">>> Data\n",
            "   1: \n",
            "   2: ###maka sure to change the stock ticker here \n",
            "   3: quandl.ApiConfig.api_key = \"uRMo697HgMj91ZZZa2_v\"\n",
            "   4: df = quandl.get_table('WIKI/PRICES', qopts = { 'columns': ['ticker', 'date', 'adj_close','open','high','low','close'] }, ticker = ['DIS'], date = { 'gte': '2009-01-01', 'lte': '2019-12-19' })\n",
            "   5: df = pd_reader.DataReader('DIS','yahoo')\n",
            "   6: df = df[['Adj Close','High','Open','Close','Low']] \n",
            "   7: df = df.dropna()\n",
            "   8: \n",
            "   9: factor_ratio = 0.7\n",
            "  10: df1 = df.iloc[:round(len(df)*factor_ratio)]\n",
            "  11: df2 = df[round(len(df)*factor_ratio):]\n",
            "  12: \n",
            "  13: training_set = df1.iloc[:,0:].values\n",
            "  14: sc = MinMaxScaler(feature_range = (0, 1))\n",
            "  15: training_set_scaled = sc.fit_transform(training_set)\n",
            "  16: \n",
            "  17: #train set\n",
            "  18: X_train = []\n",
            "  19: y_train = []\n",
            "  20: for i in range(45, len(training_set_scaled)):\n",
            "  21:     X_train.append(training_set_scaled[i-45:i])\n",
            "  22:     y_train.append(training_set_scaled[i,0])\n",
            "  23: \n",
            "  24: X_train, y_train = np.array(X_train), np.array(y_train)\n",
            "  25: X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 5))\n",
            "  26: \n",
            "  27: #test set\n",
            "  28: y_test = df2.iloc[:,0:1].values\n",
            "  29: \n",
            "  30: dataset_total = df.iloc[:,0:]\n",
            "  31: inputs = dataset_total[len(dataset_total) - len(df2) -45:].values\n",
            "  32: inputs = sc.fit_transform(inputs)\n",
            "  33: \n",
            "  34: X_test = []\n",
            "  35: for i in range(45, len(inputs)):\n",
            "  36:     X_test.append(inputs[i-45:i])\n",
            "  37: \n",
            "  38: X_test, y_test = np.array(X_test), np.array(y_test)\n",
            "  39: X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 5))\n",
            "  40: \n",
            "  41: \n",
            "  42: \n",
            "  43: \n",
            ">>> Resulting replaced keras model:\n",
            "\n",
            "   1: def keras_fmin_fnct(space):\n",
            "   2: \n",
            "   3:     \"\"\"\n",
            "   4:     Model providing function:\n",
            "   5: \n",
            "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
            "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
            "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
            "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
            "  10:     The last one is optional, though recommended, namely:\n",
            "  11:         - model: specify the model just created so that we can later use it again.\n",
            "  12:     \"\"\"\n",
            "  13:       \n",
            "  14:     model = Sequential()\n",
            "  15:     model.add(LSTM(units=space['units'], input_shape=(45,5), return_sequences= True))\n",
            "  16:     model.add(Dropout(rate=space['rate']))\n",
            "  17:     \n",
            "  18:     model.add(LSTM(units=space['units_1'], return_sequences= True))\n",
            "  19:     model.add(Dropout(rate=space['rate_1']))\n",
            "  20:     \n",
            "  21:     model.add(LSTM(units=space['units_2'], return_sequences= True))\n",
            "  22:     model.add(Dropout(rate=space['rate_2']))\n",
            "  23:     \n",
            "  24:     model.add(LSTM(units=space['units_3'], return_sequences= False))\n",
            "  25:     model.add(Dropout(rate=space['rate_3']))\n",
            "  26:     \n",
            "  27:     #model.add(LSTM(units=space['units_4'], return_sequences= False))\n",
            "  28:     #model.add(Dropout(rate=space['rate_4']))\n",
            "  29:     \n",
            "  30:     #model.add(LSTM(units=space['units_5'], return_sequences= False))\n",
            "  31:     #model.add(Dropout(rate=space['rate_5']))\n",
            "  32:     \n",
            "  33:     #model.add(LSTM(units=space['units_6']))\n",
            "  34:     #model.add(Dropout(rate=space['rate_6']))\n",
            "  35:     \n",
            "  36:     model.add(Dense(25))\n",
            "  37:     model.add(Dense(1))\n",
            "  38: \n",
            "  39:     model.compile(loss='mean_squared_error',\n",
            "  40:                   optimizer=space['optimizer'])\n",
            "  41: \n",
            "  42:     model.summary()\n",
            "  43: \n",
            "  44:     result = model.fit(X_train, y_train,\n",
            "  45:               batch_size=space['batch_size'],\n",
            "  46:               epochs=100, verbose=2)\n",
            "  47:     #get the highest validation accuracy of the training epochs\n",
            "  48:     validation_error = np.amin(result.history['loss']) \n",
            "  49:     print('Best validation error of epoch:', validation_error)\n",
            "  50:     return {'loss': -validation_error, 'status': STATUS_OK, 'model': model}\n",
            "  51: \n",
            "  0%|          | 0/4 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 45, 200)           164800    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 45, 200)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 45, 200)           320800    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 45, 200)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 45, 200)           320800    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 45, 200)           0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 300)               601200    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 25)                7525      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 26        \n",
            "=================================================================\n",
            "Total params: 1,415,151\n",
            "Trainable params: 1,415,151\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "  0%|          | 0/4 [00:01<?, ?it/s, best loss: ?]WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/100\n",
            "  0%|          | 0/4 [00:04<?, ?it/s, best loss: ?]WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            " - 28s - loss: 0.1174\n",
            "\n",
            "Epoch 2/100\n",
            " - 25s - loss: 0.0119\n",
            "\n",
            "Epoch 3/100\n",
            " - 25s - loss: 0.0036\n",
            "\n",
            "Epoch 4/100\n",
            " - 25s - loss: 0.0026\n",
            "\n",
            "Epoch 5/100\n",
            " - 25s - loss: 0.0025\n",
            "\n",
            "Epoch 6/100\n",
            " - 25s - loss: 0.0025\n",
            "\n",
            "Epoch 7/100\n",
            " - 25s - loss: 0.0024\n",
            "\n",
            "Epoch 8/100\n",
            " - 25s - loss: 0.0020\n",
            "\n",
            "Epoch 9/100\n",
            " - 25s - loss: 0.0021\n",
            "\n",
            "Epoch 10/100\n",
            " - 25s - loss: 0.0018\n",
            "\n",
            "Epoch 11/100\n",
            " - 25s - loss: 0.0024\n",
            "\n",
            "Epoch 12/100\n",
            " - 25s - loss: 0.0022\n",
            "\n",
            "Epoch 13/100\n",
            " - 24s - loss: 0.0020\n",
            "\n",
            "Epoch 14/100\n",
            " - 25s - loss: 0.0019\n",
            "\n",
            "Epoch 15/100\n",
            " - 25s - loss: 0.0023\n",
            "\n",
            "Epoch 16/100\n",
            " - 24s - loss: 0.0023\n",
            "\n",
            "Epoch 17/100\n",
            " - 25s - loss: 0.0017\n",
            "\n",
            "Epoch 18/100\n",
            " - 25s - loss: 0.0022\n",
            "\n",
            "Epoch 19/100\n",
            " - 25s - loss: 0.0018\n",
            "\n",
            "Epoch 20/100\n",
            " - 24s - loss: 0.0018\n",
            "\n",
            "Epoch 21/100\n",
            " - 25s - loss: 0.0017\n",
            "\n",
            "Epoch 22/100\n",
            " - 25s - loss: 0.0016\n",
            "\n",
            "Epoch 23/100\n",
            " - 24s - loss: 0.0016\n",
            "\n",
            "Epoch 24/100\n",
            " - 25s - loss: 0.0019\n",
            "\n",
            "Epoch 25/100\n",
            " - 24s - loss: 0.0016\n",
            "\n",
            "Epoch 26/100\n",
            " - 24s - loss: 0.0016\n",
            "\n",
            "Epoch 27/100\n",
            " - 25s - loss: 0.0017\n",
            "\n",
            "Epoch 28/100\n",
            " - 25s - loss: 0.0015\n",
            "\n",
            "Epoch 29/100\n",
            " - 24s - loss: 0.0015\n",
            "\n",
            "Epoch 30/100\n",
            " - 25s - loss: 0.0014\n",
            "\n",
            "Epoch 31/100\n",
            " - 25s - loss: 0.0014\n",
            "\n",
            "Epoch 32/100\n",
            " - 25s - loss: 0.0016\n",
            "\n",
            "Epoch 33/100\n",
            " - 25s - loss: 0.0019\n",
            "\n",
            "Epoch 34/100\n",
            " - 25s - loss: 0.0014\n",
            "\n",
            "Epoch 35/100\n",
            " - 25s - loss: 0.0014\n",
            "\n",
            "Epoch 36/100\n",
            " - 25s - loss: 0.0014\n",
            "\n",
            "Epoch 37/100\n",
            " - 25s - loss: 0.0013\n",
            "\n",
            "Epoch 38/100\n",
            " - 25s - loss: 0.0015\n",
            "\n",
            "Epoch 39/100\n",
            " - 25s - loss: 0.0019\n",
            "\n",
            "Epoch 40/100\n",
            " - 25s - loss: 0.0014\n",
            "\n",
            "Epoch 41/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 42/100\n",
            " - 24s - loss: 0.0014\n",
            "\n",
            "Epoch 43/100\n",
            " - 24s - loss: 0.0013\n",
            "\n",
            "Epoch 44/100\n",
            " - 24s - loss: 0.0013\n",
            "\n",
            "Epoch 45/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 46/100\n",
            " - 25s - loss: 0.0012\n",
            "\n",
            "Epoch 47/100\n",
            " - 25s - loss: 0.0014\n",
            "\n",
            "Epoch 48/100\n",
            " - 24s - loss: 0.0018\n",
            "\n",
            "Epoch 49/100\n",
            " - 24s - loss: 0.0012\n",
            "\n",
            "Epoch 50/100\n",
            " - 25s - loss: 0.0013\n",
            "\n",
            "Epoch 51/100\n",
            " - 24s - loss: 0.0014\n",
            "\n",
            "Epoch 52/100\n",
            " - 24s - loss: 0.0012\n",
            "\n",
            "Epoch 53/100\n",
            " - 25s - loss: 0.0010\n",
            "\n",
            "Epoch 54/100\n",
            " - 24s - loss: 0.0012\n",
            "\n",
            "Epoch 55/100\n",
            " - 24s - loss: 0.0010\n",
            "\n",
            "Epoch 56/100\n",
            " - 24s - loss: 0.0012\n",
            "\n",
            "Epoch 57/100\n",
            " - 24s - loss: 0.0012\n",
            "\n",
            "Epoch 58/100\n",
            " - 25s - loss: 0.0011\n",
            "\n",
            "Epoch 59/100\n",
            " - 24s - loss: 0.0013\n",
            "\n",
            "Epoch 60/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 61/100\n",
            " - 24s - loss: 0.0010\n",
            "\n",
            "Epoch 62/100\n",
            " - 24s - loss: 0.0013\n",
            "\n",
            "Epoch 63/100\n",
            " - 24s - loss: 0.0010\n",
            "\n",
            "Epoch 64/100\n",
            " - 24s - loss: 0.0010\n",
            "\n",
            "Epoch 65/100\n",
            " - 24s - loss: 0.0010\n",
            "\n",
            "Epoch 66/100\n",
            " - 24s - loss: 0.0010\n",
            "\n",
            "Epoch 67/100\n",
            " - 24s - loss: 9.7585e-04\n",
            "\n",
            "Epoch 68/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 69/100\n",
            " - 24s - loss: 0.0010\n",
            "\n",
            "Epoch 70/100\n",
            " - 24s - loss: 9.4963e-04\n",
            "\n",
            "Epoch 71/100\n",
            " - 24s - loss: 8.9338e-04\n",
            "\n",
            "Epoch 72/100\n",
            " - 24s - loss: 8.2174e-04\n",
            "\n",
            "Epoch 73/100\n",
            " - 24s - loss: 8.6886e-04\n",
            "\n",
            "Epoch 74/100\n",
            " - 24s - loss: 8.5959e-04\n",
            "\n",
            "Epoch 75/100\n",
            " - 24s - loss: 9.6150e-04\n",
            "\n",
            "Epoch 76/100\n",
            " - 25s - loss: 8.3453e-04\n",
            "\n",
            "Epoch 77/100\n",
            " - 24s - loss: 8.4630e-04\n",
            "\n",
            "Epoch 78/100\n",
            " - 24s - loss: 9.6603e-04\n",
            "\n",
            "Epoch 79/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 80/100\n",
            " - 24s - loss: 8.4822e-04\n",
            "\n",
            "Epoch 81/100\n",
            " - 24s - loss: 9.0542e-04\n",
            "\n",
            "Epoch 82/100\n",
            " - 24s - loss: 9.7227e-04\n",
            "\n",
            "Epoch 83/100\n",
            " - 24s - loss: 8.4634e-04\n",
            "\n",
            "Epoch 84/100\n",
            " - 24s - loss: 8.7525e-04\n",
            "\n",
            "Epoch 85/100\n",
            " - 24s - loss: 9.0373e-04\n",
            "\n",
            "Epoch 86/100\n",
            " - 24s - loss: 8.4116e-04\n",
            "\n",
            "Epoch 87/100\n",
            " - 24s - loss: 0.0010\n",
            "\n",
            "Epoch 88/100\n",
            " - 24s - loss: 9.4555e-04\n",
            "\n",
            "Epoch 89/100\n",
            " - 24s - loss: 8.7871e-04\n",
            "\n",
            "Epoch 90/100\n",
            " - 25s - loss: 8.6396e-04\n",
            "\n",
            "Epoch 91/100\n",
            " - 24s - loss: 8.1304e-04\n",
            "\n",
            "Epoch 92/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 93/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 94/100\n",
            " - 24s - loss: 9.8410e-04\n",
            "\n",
            "Epoch 95/100\n",
            " - 24s - loss: 0.0012\n",
            "\n",
            "Epoch 96/100\n",
            " - 24s - loss: 9.3747e-04\n",
            "\n",
            "Epoch 97/100\n",
            " - 25s - loss: 9.1962e-04\n",
            "\n",
            "Epoch 98/100\n",
            " - 24s - loss: 8.7580e-04\n",
            "\n",
            "Epoch 99/100\n",
            " - 24s - loss: 8.6010e-04\n",
            "\n",
            "Epoch 100/100\n",
            " - 24s - loss: 7.6836e-04\n",
            "\n",
            "Best validation error of epoch:\n",
            "0.0007683574387371756\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_5 (LSTM)                (None, 45, 300)           367200    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 45, 300)           0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 45, 200)           400800    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 45, 200)           0         \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 45, 300)           601200    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 45, 300)           0         \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 25)                2525      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 26        \n",
            "=================================================================\n",
            "Total params: 1,532,151\n",
            "Trainable params: 1,532,151\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            " - 21s - loss: 0.1292\n",
            "\n",
            "Epoch 2/100\n",
            " - 19s - loss: 0.0230\n",
            "\n",
            "Epoch 3/100\n",
            " - 18s - loss: 0.0143\n",
            "\n",
            "Epoch 4/100\n",
            " - 18s - loss: 0.0069\n",
            "\n",
            "Epoch 5/100\n",
            " - 18s - loss: 0.0046\n",
            "\n",
            "Epoch 6/100\n",
            " - 18s - loss: 0.0035\n",
            "\n",
            "Epoch 7/100\n",
            " - 18s - loss: 0.0031\n",
            "\n",
            "Epoch 8/100\n",
            " - 18s - loss: 0.0027\n",
            "\n",
            "Epoch 9/100\n",
            " - 19s - loss: 0.0023\n",
            "\n",
            "Epoch 10/100\n",
            " - 19s - loss: 0.0026\n",
            "\n",
            "Epoch 11/100\n",
            " - 19s - loss: 0.0023\n",
            "\n",
            "Epoch 12/100\n",
            " - 19s - loss: 0.0024\n",
            "\n",
            "Epoch 13/100\n",
            " - 18s - loss: 0.0023\n",
            "\n",
            "Epoch 14/100\n",
            " - 18s - loss: 0.0020\n",
            "\n",
            "Epoch 15/100\n",
            " - 19s - loss: 0.0022\n",
            "\n",
            "Epoch 16/100\n",
            " - 19s - loss: 0.0022\n",
            "\n",
            "Epoch 17/100\n",
            " - 18s - loss: 0.0025\n",
            "\n",
            "Epoch 18/100\n",
            " - 18s - loss: 0.0024\n",
            "\n",
            "Epoch 19/100\n",
            " - 19s - loss: 0.0021\n",
            "\n",
            "Epoch 20/100\n",
            " - 20s - loss: 0.0019\n",
            "\n",
            "Epoch 21/100\n",
            " - 19s - loss: 0.0020\n",
            "\n",
            "Epoch 22/100\n",
            " - 19s - loss: 0.0020\n",
            "\n",
            "Epoch 23/100\n",
            " - 19s - loss: 0.0021\n",
            "\n",
            "Epoch 24/100\n",
            " - 19s - loss: 0.0020\n",
            "\n",
            "Epoch 25/100\n",
            " - 19s - loss: 0.0019\n",
            "\n",
            "Epoch 26/100\n",
            " - 19s - loss: 0.0018\n",
            "\n",
            "Epoch 27/100\n",
            " - 19s - loss: 0.0018\n",
            "\n",
            "Epoch 28/100\n",
            " - 19s - loss: 0.0020\n",
            "\n",
            "Epoch 29/100\n",
            " - 19s - loss: 0.0020\n",
            "\n",
            "Epoch 30/100\n",
            " - 19s - loss: 0.0018\n",
            "\n",
            "Epoch 31/100\n",
            " - 19s - loss: 0.0019\n",
            "\n",
            "Epoch 32/100\n",
            " - 19s - loss: 0.0020\n",
            "\n",
            "Epoch 33/100\n",
            " - 19s - loss: 0.0020\n",
            "\n",
            "Epoch 34/100\n",
            " - 19s - loss: 0.0017\n",
            "\n",
            "Epoch 35/100\n",
            " - 19s - loss: 0.0019\n",
            "\n",
            "Epoch 36/100\n",
            " - 19s - loss: 0.0017\n",
            "\n",
            "Epoch 37/100\n",
            " - 19s - loss: 0.0017\n",
            "\n",
            "Epoch 38/100\n",
            " - 19s - loss: 0.0018\n",
            "\n",
            "Epoch 39/100\n",
            " - 19s - loss: 0.0017\n",
            "\n",
            "Epoch 40/100\n",
            " - 19s - loss: 0.0016\n",
            "\n",
            "Epoch 41/100\n",
            " - 19s - loss: 0.0016\n",
            "\n",
            "Epoch 42/100\n",
            " - 19s - loss: 0.0017\n",
            "\n",
            "Epoch 43/100\n",
            " - 19s - loss: 0.0015\n",
            "\n",
            "Epoch 44/100\n",
            " - 19s - loss: 0.0016\n",
            "\n",
            "Epoch 45/100\n",
            " - 19s - loss: 0.0018\n",
            "\n",
            "Epoch 46/100\n",
            " - 19s - loss: 0.0018\n",
            "\n",
            "Epoch 47/100\n",
            " - 19s - loss: 0.0018\n",
            "\n",
            "Epoch 48/100\n",
            " - 19s - loss: 0.0016\n",
            "\n",
            "Epoch 49/100\n",
            " - 19s - loss: 0.0018\n",
            "\n",
            "Epoch 50/100\n",
            " - 19s - loss: 0.0016\n",
            "\n",
            "Epoch 51/100\n",
            " - 19s - loss: 0.0017\n",
            "\n",
            "Epoch 52/100\n",
            " - 19s - loss: 0.0015\n",
            "\n",
            "Epoch 53/100\n",
            " - 19s - loss: 0.0015\n",
            "\n",
            "Epoch 54/100\n",
            " - 19s - loss: 0.0016\n",
            "\n",
            "Epoch 55/100\n",
            " - 19s - loss: 0.0014\n",
            "\n",
            "Epoch 56/100\n",
            " - 19s - loss: 0.0015\n",
            "\n",
            "Epoch 57/100\n",
            " - 19s - loss: 0.0015\n",
            "\n",
            "Epoch 58/100\n",
            " - 19s - loss: 0.0016\n",
            "\n",
            "Epoch 59/100\n",
            " - 19s - loss: 0.0014\n",
            "\n",
            "Epoch 60/100\n",
            " - 19s - loss: 0.0013\n",
            "\n",
            "Epoch 61/100\n",
            " - 19s - loss: 0.0015\n",
            "\n",
            "Epoch 62/100\n",
            " - 19s - loss: 0.0015\n",
            "\n",
            "Epoch 63/100\n",
            " - 19s - loss: 0.0014\n",
            "\n",
            "Epoch 64/100\n",
            " - 19s - loss: 0.0018\n",
            "\n",
            "Epoch 65/100\n",
            " - 19s - loss: 0.0020\n",
            "\n",
            "Epoch 66/100\n",
            " - 19s - loss: 0.0015\n",
            "\n",
            "Epoch 67/100\n",
            " - 19s - loss: 0.0014\n",
            "\n",
            "Epoch 68/100\n",
            " - 19s - loss: 0.0015\n",
            "\n",
            "Epoch 69/100\n",
            " - 19s - loss: 0.0014\n",
            "\n",
            "Epoch 70/100\n",
            " - 19s - loss: 0.0014\n",
            "\n",
            "Epoch 71/100\n",
            " - 19s - loss: 0.0015\n",
            "\n",
            "Epoch 72/100\n",
            " - 19s - loss: 0.0013\n",
            "\n",
            "Epoch 73/100\n",
            " - 19s - loss: 0.0013\n",
            "\n",
            "Epoch 74/100\n",
            " - 19s - loss: 0.0014\n",
            "\n",
            "Epoch 75/100\n",
            " - 19s - loss: 0.0013\n",
            "\n",
            "Epoch 76/100\n",
            " - 19s - loss: 0.0013\n",
            "\n",
            "Epoch 77/100\n",
            " - 19s - loss: 0.0013\n",
            "\n",
            "Epoch 78/100\n",
            " - 19s - loss: 0.0013\n",
            "\n",
            "Epoch 79/100\n",
            " - 19s - loss: 0.0015\n",
            "\n",
            "Epoch 80/100\n",
            " - 19s - loss: 0.0012\n",
            "\n",
            "Epoch 81/100\n",
            " - 19s - loss: 0.0013\n",
            "\n",
            "Epoch 82/100\n",
            " - 19s - loss: 0.0014\n",
            "\n",
            "Epoch 83/100\n",
            " - 19s - loss: 0.0013\n",
            "\n",
            "Epoch 84/100\n",
            " - 19s - loss: 0.0012\n",
            "\n",
            "Epoch 85/100\n",
            " - 19s - loss: 0.0012\n",
            "\n",
            "Epoch 86/100\n",
            " - 19s - loss: 0.0013\n",
            "\n",
            "Epoch 87/100\n",
            " - 19s - loss: 0.0013\n",
            "\n",
            "Epoch 88/100\n",
            " - 19s - loss: 0.0014\n",
            "\n",
            "Epoch 89/100\n",
            " - 19s - loss: 0.0013\n",
            "\n",
            "Epoch 90/100\n",
            " - 19s - loss: 0.0014\n",
            "\n",
            "Epoch 91/100\n",
            " - 19s - loss: 0.0014\n",
            "\n",
            "Epoch 92/100\n",
            " - 19s - loss: 0.0015\n",
            "\n",
            "Epoch 93/100\n",
            " - 19s - loss: 0.0013\n",
            "\n",
            "Epoch 94/100\n",
            " - 19s - loss: 0.0012\n",
            "\n",
            "Epoch 95/100\n",
            " - 19s - loss: 0.0014\n",
            "\n",
            "Epoch 96/100\n",
            " - 19s - loss: 0.0013\n",
            "\n",
            "Epoch 97/100\n",
            " - 19s - loss: 0.0013\n",
            "\n",
            "Epoch 98/100\n",
            " - 19s - loss: 0.0013\n",
            "\n",
            "Epoch 99/100\n",
            " - 19s - loss: 0.0014\n",
            "\n",
            "Epoch 100/100\n",
            " - 19s - loss: 0.0014\n",
            "\n",
            "Best validation error of epoch:\n",
            "0.0012018645580426075\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_9 (LSTM)                (None, 45, 100)           42400     \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 45, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 45, 300)           481200    \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 45, 300)           0         \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 45, 200)           400800    \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 45, 200)           0         \n",
            "_________________________________________________________________\n",
            "lstm_12 (LSTM)               (None, 100)               120400    \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 25)                2525      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 26        \n",
            "=================================================================\n",
            "Total params: 1,047,351\n",
            "Trainable params: 1,047,351\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            " - 22s - loss: 0.0374\n",
            "\n",
            "Epoch 2/100\n",
            " - 19s - loss: 0.0058\n",
            "\n",
            "Epoch 3/100\n",
            " - 19s - loss: 0.0044\n",
            "\n",
            "Epoch 4/100\n",
            " - 19s - loss: 0.0043\n",
            "\n",
            "Epoch 5/100\n",
            " - 19s - loss: 0.0037\n",
            "\n",
            "Epoch 6/100\n",
            " - 19s - loss: 0.0035\n",
            "\n",
            "Epoch 7/100\n",
            " - 19s - loss: 0.0038\n",
            "\n",
            "Epoch 8/100\n",
            " - 19s - loss: 0.0046\n",
            "\n",
            "Epoch 9/100\n",
            " - 19s - loss: 0.0033\n",
            "\n",
            "Epoch 10/100\n",
            " - 19s - loss: 0.0030\n",
            "\n",
            "Epoch 11/100\n",
            " - 19s - loss: 0.0027\n",
            "\n",
            "Epoch 12/100\n",
            " - 19s - loss: 0.0027\n",
            "\n",
            "Epoch 13/100\n",
            " - 19s - loss: 0.0024\n",
            "\n",
            "Epoch 14/100\n",
            " - 19s - loss: 0.0027\n",
            "\n",
            "Epoch 15/100\n",
            " - 19s - loss: 0.0023\n",
            "\n",
            "Epoch 16/100\n",
            " - 19s - loss: 0.0027\n",
            "\n",
            "Epoch 17/100\n",
            " - 19s - loss: 0.0033\n",
            "\n",
            "Epoch 18/100\n",
            " - 19s - loss: 0.0025\n",
            "\n",
            "Epoch 19/100\n",
            " - 19s - loss: 0.0022\n",
            "\n",
            "Epoch 20/100\n",
            " - 19s - loss: 0.0024\n",
            "\n",
            "Epoch 21/100\n",
            " - 19s - loss: 0.0027\n",
            "\n",
            "Epoch 22/100\n",
            " - 19s - loss: 0.0020\n",
            "\n",
            "Epoch 23/100\n",
            " - 19s - loss: 0.0020\n",
            "\n",
            "Epoch 24/100\n",
            " - 19s - loss: 0.0018\n",
            "\n",
            "Epoch 25/100\n",
            " - 19s - loss: 0.0021\n",
            "\n",
            "Epoch 26/100\n",
            " - 19s - loss: 0.0021\n",
            "\n",
            "Epoch 27/100\n",
            " - 19s - loss: 0.0019\n",
            "\n",
            "Epoch 28/100\n",
            " - 19s - loss: 0.0017\n",
            "\n",
            "Epoch 29/100\n",
            " - 19s - loss: 0.0017\n",
            "\n",
            "Epoch 30/100\n",
            " - 19s - loss: 0.0018\n",
            "\n",
            "Epoch 31/100\n",
            " - 19s - loss: 0.0018\n",
            "\n",
            "Epoch 32/100\n",
            " - 19s - loss: 0.0016\n",
            "\n",
            "Epoch 33/100\n",
            " - 19s - loss: 0.0018\n",
            "\n",
            "Epoch 34/100\n",
            " - 18s - loss: 0.0017\n",
            "\n",
            "Epoch 35/100\n",
            " - 19s - loss: 0.0019\n",
            "\n",
            "Epoch 36/100\n",
            " - 18s - loss: 0.0016\n",
            "\n",
            "Epoch 37/100\n",
            " - 18s - loss: 0.0018\n",
            "\n",
            "Epoch 38/100\n",
            " - 18s - loss: 0.0018\n",
            "\n",
            "Epoch 39/100\n",
            " - 19s - loss: 0.0016\n",
            "\n",
            "Epoch 40/100\n",
            " - 18s - loss: 0.0016\n",
            "\n",
            "Epoch 41/100\n",
            " - 18s - loss: 0.0015\n",
            "\n",
            "Epoch 42/100\n",
            " - 18s - loss: 0.0016\n",
            "\n",
            "Epoch 43/100\n",
            " - 18s - loss: 0.0019\n",
            "\n",
            "Epoch 44/100\n",
            " - 18s - loss: 0.0016\n",
            "\n",
            "Epoch 45/100\n",
            " - 18s - loss: 0.0019\n",
            "\n",
            "Epoch 46/100\n",
            " - 18s - loss: 0.0016\n",
            "\n",
            "Epoch 47/100\n",
            " - 18s - loss: 0.0017\n",
            "\n",
            "Epoch 48/100\n",
            " - 18s - loss: 0.0017\n",
            "\n",
            "Epoch 49/100\n",
            " - 19s - loss: 0.0016\n",
            "\n",
            "Epoch 50/100\n",
            " - 18s - loss: 0.0015\n",
            "\n",
            "Epoch 51/100\n",
            " - 18s - loss: 0.0014\n",
            "\n",
            "Epoch 52/100\n",
            " - 19s - loss: 0.0017\n",
            "\n",
            "Epoch 53/100\n",
            " - 18s - loss: 0.0015\n",
            "\n",
            "Epoch 54/100\n",
            " - 18s - loss: 0.0016\n",
            "\n",
            "Epoch 55/100\n",
            " - 18s - loss: 0.0015\n",
            "\n",
            "Epoch 56/100\n",
            " - 18s - loss: 0.0015\n",
            "\n",
            "Epoch 57/100\n",
            " - 18s - loss: 0.0016\n",
            "\n",
            "Epoch 58/100\n",
            " - 18s - loss: 0.0014\n",
            "\n",
            "Epoch 59/100\n",
            " - 18s - loss: 0.0015\n",
            "\n",
            "Epoch 60/100\n",
            " - 18s - loss: 0.0015\n",
            "\n",
            "Epoch 61/100\n",
            " - 18s - loss: 0.0015\n",
            "\n",
            "Epoch 62/100\n",
            " - 18s - loss: 0.0013\n",
            "\n",
            "Epoch 63/100\n",
            " - 18s - loss: 0.0016\n",
            "\n",
            "Epoch 64/100\n",
            " - 18s - loss: 0.0017\n",
            "\n",
            "Epoch 65/100\n",
            " - 18s - loss: 0.0015\n",
            "\n",
            "Epoch 66/100\n",
            " - 19s - loss: 0.0014\n",
            "\n",
            "Epoch 67/100\n",
            " - 18s - loss: 0.0015\n",
            "\n",
            "Epoch 68/100\n",
            " - 18s - loss: 0.0015\n",
            "\n",
            "Epoch 69/100\n",
            " - 19s - loss: 0.0016\n",
            "\n",
            "Epoch 70/100\n",
            " - 18s - loss: 0.0015\n",
            "\n",
            "Epoch 71/100\n",
            " - 18s - loss: 0.0016\n",
            "\n",
            "Epoch 72/100\n",
            " - 18s - loss: 0.0014\n",
            "\n",
            "Epoch 73/100\n",
            " - 18s - loss: 0.0013\n",
            "\n",
            "Epoch 74/100\n",
            " - 18s - loss: 0.0014\n",
            "\n",
            "Epoch 75/100\n",
            " - 18s - loss: 0.0017\n",
            "\n",
            "Epoch 76/100\n",
            " - 18s - loss: 0.0016\n",
            "\n",
            "Epoch 77/100\n",
            " - 18s - loss: 0.0015\n",
            "\n",
            "Epoch 78/100\n",
            " - 18s - loss: 0.0013\n",
            "\n",
            "Epoch 79/100\n",
            " - 18s - loss: 0.0013\n",
            "\n",
            "Epoch 80/100\n",
            " - 18s - loss: 0.0014\n",
            "\n",
            "Epoch 81/100\n",
            " - 18s - loss: 0.0014\n",
            "\n",
            "Epoch 82/100\n",
            " - 18s - loss: 0.0014\n",
            "\n",
            "Epoch 83/100\n",
            " - 18s - loss: 0.0015\n",
            "\n",
            "Epoch 84/100\n",
            " - 18s - loss: 0.0014\n",
            "\n",
            "Epoch 85/100\n",
            " - 18s - loss: 0.0017\n",
            "\n",
            "Epoch 86/100\n",
            " - 19s - loss: 0.0015\n",
            "\n",
            "Epoch 87/100\n",
            " - 18s - loss: 0.0014\n",
            "\n",
            "Epoch 88/100\n",
            " - 18s - loss: 0.0014\n",
            "\n",
            "Epoch 89/100\n",
            " - 18s - loss: 0.0013\n",
            "\n",
            "Epoch 90/100\n",
            " - 18s - loss: 0.0013\n",
            "\n",
            "Epoch 91/100\n",
            " - 18s - loss: 0.0014\n",
            "\n",
            "Epoch 92/100\n",
            " - 18s - loss: 0.0014\n",
            "\n",
            "Epoch 93/100\n",
            " - 18s - loss: 0.0013\n",
            "\n",
            "Epoch 94/100\n",
            " - 18s - loss: 0.0013\n",
            "\n",
            "Epoch 95/100\n",
            " - 18s - loss: 0.0014\n",
            "\n",
            "Epoch 96/100\n",
            " - 18s - loss: 0.0013\n",
            "\n",
            "Epoch 97/100\n",
            " - 18s - loss: 0.0012\n",
            "\n",
            "Epoch 98/100\n",
            " - 18s - loss: 0.0013\n",
            "\n",
            "Epoch 99/100\n",
            " - 18s - loss: 0.0014\n",
            "\n",
            "Epoch 100/100\n",
            " - 18s - loss: 0.0013\n",
            "\n",
            "Best validation error of epoch:\n",
            "0.0012483843902557873\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_13 (LSTM)               (None, 45, 300)           367200    \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 45, 300)           0         \n",
            "_________________________________________________________________\n",
            "lstm_14 (LSTM)               (None, 45, 100)           160400    \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 45, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_15 (LSTM)               (None, 45, 300)           481200    \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 45, 300)           0         \n",
            "_________________________________________________________________\n",
            "lstm_16 (LSTM)               (None, 200)               400800    \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 25)                5025      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 26        \n",
            "=================================================================\n",
            "Total params: 1,414,651\n",
            "Trainable params: 1,414,651\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            " - 27s - loss: 0.0729\n",
            "\n",
            "Epoch 2/100\n",
            " - 24s - loss: 0.0083\n",
            "\n",
            "Epoch 3/100\n",
            " - 24s - loss: 0.0035\n",
            "\n",
            "Epoch 4/100\n",
            " - 24s - loss: 0.0025\n",
            "\n",
            "Epoch 5/100\n",
            " - 24s - loss: 0.0022\n",
            "\n",
            "Epoch 6/100\n",
            " - 24s - loss: 0.0020\n",
            "\n",
            "Epoch 7/100\n",
            " - 24s - loss: 0.0023\n",
            "\n",
            "Epoch 8/100\n",
            " - 24s - loss: 0.0021\n",
            "\n",
            "Epoch 9/100\n",
            " - 24s - loss: 0.0021\n",
            "\n",
            "Epoch 10/100\n",
            " - 24s - loss: 0.0019\n",
            "\n",
            "Epoch 11/100\n",
            " - 24s - loss: 0.0019\n",
            "\n",
            "Epoch 12/100\n",
            " - 24s - loss: 0.0016\n",
            "\n",
            "Epoch 13/100\n",
            " - 24s - loss: 0.0019\n",
            "\n",
            "Epoch 14/100\n",
            " - 24s - loss: 0.0021\n",
            "\n",
            "Epoch 15/100\n",
            " - 24s - loss: 0.0016\n",
            "\n",
            "Epoch 16/100\n",
            " - 24s - loss: 0.0016\n",
            "\n",
            "Epoch 17/100\n",
            " - 24s - loss: 0.0015\n",
            "\n",
            "Epoch 18/100\n",
            " - 24s - loss: 0.0016\n",
            "\n",
            "Epoch 19/100\n",
            " - 24s - loss: 0.0018\n",
            "\n",
            "Epoch 20/100\n",
            " - 24s - loss: 0.0015\n",
            "\n",
            "Epoch 21/100\n",
            " - 24s - loss: 0.0015\n",
            "\n",
            "Epoch 22/100\n",
            " - 24s - loss: 0.0016\n",
            "\n",
            "Epoch 23/100\n",
            " - 24s - loss: 0.0018\n",
            "\n",
            "Epoch 24/100\n",
            " - 24s - loss: 0.0020\n",
            "\n",
            "Epoch 25/100\n",
            " - 24s - loss: 0.0018\n",
            "\n",
            "Epoch 26/100\n",
            " - 24s - loss: 0.0016\n",
            "\n",
            "Epoch 27/100\n",
            " - 24s - loss: 0.0015\n",
            "\n",
            "Epoch 28/100\n",
            " - 24s - loss: 0.0015\n",
            "\n",
            "Epoch 29/100\n",
            " - 24s - loss: 0.0013\n",
            "\n",
            "Epoch 30/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 31/100\n",
            " - 24s - loss: 0.0014\n",
            "\n",
            "Epoch 32/100\n",
            " - 24s - loss: 0.0014\n",
            "\n",
            "Epoch 33/100\n",
            " - 24s - loss: 0.0012\n",
            "\n",
            "Epoch 34/100\n",
            " - 24s - loss: 0.0012\n",
            "\n",
            "Epoch 35/100\n",
            " - 24s - loss: 0.0012\n",
            "\n",
            "Epoch 36/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 37/100\n",
            " - 24s - loss: 0.0013\n",
            "\n",
            "Epoch 38/100\n",
            " - 24s - loss: 0.0015\n",
            "\n",
            "Epoch 39/100\n",
            " - 24s - loss: 0.0013\n",
            "\n",
            "Epoch 40/100\n",
            " - 24s - loss: 0.0012\n",
            "\n",
            "Epoch 41/100\n",
            " - 24s - loss: 0.0013\n",
            "\n",
            "Epoch 42/100\n",
            " - 24s - loss: 0.0013\n",
            "\n",
            "Epoch 43/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 44/100\n",
            " - 24s - loss: 0.0013\n",
            "\n",
            "Epoch 45/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 46/100\n",
            " - 24s - loss: 9.5671e-04\n",
            "\n",
            "Epoch 47/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 48/100\n",
            " - 24s - loss: 0.0012\n",
            "\n",
            "Epoch 49/100\n",
            " - 24s - loss: 0.0015\n",
            "\n",
            "Epoch 50/100\n",
            " - 24s - loss: 0.0016\n",
            "\n",
            "Epoch 51/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 52/100\n",
            " - 24s - loss: 0.0010\n",
            "\n",
            "Epoch 53/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 54/100\n",
            " - 25s - loss: 0.0012\n",
            "\n",
            "Epoch 55/100\n",
            " - 24s - loss: 0.0010\n",
            "\n",
            "Epoch 56/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 57/100\n",
            " - 24s - loss: 0.0013\n",
            "\n",
            "Epoch 58/100\n",
            " - 24s - loss: 0.0010\n",
            "\n",
            "Epoch 59/100\n",
            " - 24s - loss: 0.0015\n",
            "\n",
            "Epoch 60/100\n",
            " - 24s - loss: 9.9710e-04\n",
            "\n",
            "Epoch 61/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 62/100\n",
            " - 24s - loss: 8.9156e-04\n",
            "\n",
            "Epoch 63/100\n",
            " - 24s - loss: 9.8523e-04\n",
            "\n",
            "Epoch 64/100\n",
            " - 24s - loss: 0.0012\n",
            "\n",
            "Epoch 65/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 66/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 67/100\n",
            " - 24s - loss: 9.7923e-04\n",
            "\n",
            "Epoch 68/100\n",
            " - 24s - loss: 9.1310e-04\n",
            "\n",
            "Epoch 69/100\n",
            " - 24s - loss: 8.1361e-04\n",
            "\n",
            "Epoch 70/100\n",
            " - 24s - loss: 9.8542e-04\n",
            "\n",
            "Epoch 71/100\n",
            " - 24s - loss: 9.6274e-04\n",
            "\n",
            "Epoch 72/100\n",
            " - 24s - loss: 8.6015e-04\n",
            "\n",
            "Epoch 73/100\n",
            " - 24s - loss: 8.4518e-04\n",
            "\n",
            "Epoch 74/100\n",
            " - 24s - loss: 0.0011\n",
            "\n",
            "Epoch 75/100\n",
            " - 24s - loss: 8.7736e-04\n",
            "\n",
            "Epoch 76/100\n",
            " - 24s - loss: 8.9257e-04\n",
            "\n",
            "Epoch 77/100\n",
            " - 24s - loss: 8.1640e-04\n",
            "\n",
            "Epoch 78/100\n",
            " - 24s - loss: 7.8763e-04\n",
            "\n",
            "Epoch 79/100\n",
            " - 24s - loss: 9.4359e-04\n",
            "\n",
            "Epoch 80/100\n",
            " - 24s - loss: 7.6618e-04\n",
            "\n",
            "Epoch 81/100\n",
            " - 24s - loss: 8.7971e-04\n",
            "\n",
            "Epoch 82/100\n",
            " - 24s - loss: 0.0010\n",
            "\n",
            "Epoch 83/100\n",
            " - 24s - loss: 7.9933e-04\n",
            "\n",
            "Epoch 84/100\n",
            " - 24s - loss: 8.0063e-04\n",
            "\n",
            "Epoch 85/100\n",
            " - 24s - loss: 8.3077e-04\n",
            "\n",
            "Epoch 86/100\n",
            " - 24s - loss: 8.1365e-04\n",
            "\n",
            "Epoch 87/100\n",
            " - 24s - loss: 7.8293e-04\n",
            "\n",
            "Epoch 88/100\n",
            " - 24s - loss: 8.0702e-04\n",
            "\n",
            "Epoch 89/100\n",
            " - 24s - loss: 8.7117e-04\n",
            "\n",
            "Epoch 90/100\n",
            " - 24s - loss: 7.7740e-04\n",
            "\n",
            "Epoch 91/100\n",
            " - 24s - loss: 9.2609e-04\n",
            "\n",
            "Epoch 92/100\n",
            " - 23s - loss: 8.9792e-04\n",
            "\n",
            "Epoch 93/100\n",
            " - 23s - loss: 7.6397e-04\n",
            "\n",
            "Epoch 94/100\n",
            " - 23s - loss: 8.1234e-04\n",
            "\n",
            "Epoch 95/100\n",
            " - 23s - loss: 7.6932e-04\n",
            "\n",
            "Epoch 96/100\n",
            " - 23s - loss: 7.6282e-04\n",
            "\n",
            "Epoch 97/100\n",
            " - 23s - loss: 7.8041e-04\n",
            "\n",
            "Epoch 98/100\n",
            " - 23s - loss: 9.9158e-04\n",
            "\n",
            "Epoch 99/100\n",
            " - 23s - loss: 8.5494e-04\n",
            "\n",
            "Epoch 100/100\n",
            " - 23s - loss: 7.9029e-04\n",
            "\n",
            "Best validation error of epoch:\n",
            "0.0007628192365898386\n",
            "100%|██████████| 4/4 [2:23:17<00:00, 2227.36s/it, best loss: -0.0012483843902557873]\n",
            "Evalutation of best performing model:\n",
            "<keras.engine.sequential.Sequential object at 0x7fb45f4aceb8>\n",
            "Best performing model chosen hyper-parameters:\n",
            "{'batch_size': 2, 'optimizer': 0, 'rate': 0.21219197235527482, 'rate_1': 0.49911883690731396, 'rate_2': 0.4112992742254813, 'rate_3': 0.4287674478496333, 'rate_4': 0.4074604059830311, 'rate_5': 0.46008729853372504, 'rate_6': 0.4915577688323514, 'units': 0, 'units_1': 2, 'units_2': 1, 'units_3': 0, 'units_4': 2, 'units_5': 2, 'units_6': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eyo4NHdqyh-n",
        "colab_type": "code",
        "outputId": "567f5240-907c-44b3-a7bc-8d0cc060817e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "best_run"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 2,\n",
              " 'optimizer': 0,\n",
              " 'rate': 0.21219197235527482,\n",
              " 'rate_1': 0.49911883690731396,\n",
              " 'rate_2': 0.4112992742254813,\n",
              " 'rate_3': 0.4287674478496333,\n",
              " 'rate_4': 0.4074604059830311,\n",
              " 'rate_5': 0.46008729853372504,\n",
              " 'rate_6': 0.4915577688323514,\n",
              " 'units': 0,\n",
              " 'units_1': 2,\n",
              " 'units_2': 1,\n",
              " 'units_3': 0,\n",
              " 'units_4': 2,\n",
              " 'units_5': 2,\n",
              " 'units_6': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7ZzZ2-PHyh-p",
        "colab_type": "code",
        "outputId": "a283efa2-be85-46c3-f575-8307aa9daf59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "best_model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_9 (LSTM)                (None, 45, 100)           42400     \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 45, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 45, 300)           481200    \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 45, 300)           0         \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 45, 200)           400800    \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 45, 200)           0         \n",
            "_________________________________________________________________\n",
            "lstm_12 (LSTM)               (None, 100)               120400    \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 25)                2525      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 26        \n",
            "=================================================================\n",
            "Total params: 1,047,351\n",
            "Trainable params: 1,047,351\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLJZi8Hjyh-q",
        "colab_type": "text"
      },
      "source": [
        "## Make sure to edit your stock ticker again in the next cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa_aX3gvyh-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##maka sure to change the stock ticker here \n",
        "quandl.ApiConfig.api_key = \"uRMo697HgMj91ZZZa2_v\"\n",
        "df = quandl.get_table('WIKI/PRICES', qopts = { 'columns': ['ticker', 'date', 'adj_close','open','high','low','close'] }, ticker = ['DIS'], date = { 'gte': '2009-01-01', 'lte': '2019-12-19' })\n",
        "df = pd_reader.DataReader('DIS','yahoo')\n",
        "df = df[['Adj Close','High','Open','Close','Low']]\n",
        "df = df.dropna()\n",
        "    \n",
        "factor_ratio = 0.7\n",
        "df1 = df.iloc[:round(len(df)*factor_ratio)]\n",
        "df2 = df[round(len(df)*factor_ratio):]\n",
        "    \n",
        "training_set = df1.iloc[:,0:].values\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "training_set_scaled = sc.fit_transform(training_set)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alb_Bonxyh-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_stock_price = df2.iloc[:,0:1].values\n",
        "\n",
        "dataset_total = df.iloc[:,0:]\n",
        "\n",
        "inputs = dataset_total[len(dataset_total) - len(df2) -45:].values\n",
        "#inputs = inputs.reshape(-1,1)\n",
        "inputs = sc.fit_transform(inputs)\n",
        "\n",
        "X_test = []\n",
        "for i in range(45, len(inputs)):\n",
        "    X_test.append(inputs[i-45:i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "414zI9ISyh-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = np.array(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO1BXdHRyh-t",
        "colab_type": "code",
        "outputId": "bfaff07d-3ebd-417c-e18f-cc71cd31536c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 5))\n",
        "X_test.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(758, 45, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VaNcjScYyh-v",
        "colab_type": "code",
        "outputId": "54cbc486-f996-4332-9573-ee490a0271cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "predicted_stock_price= best_model.predict(X_test)\n",
        "sc.fit_transform(real_stock_price)\n",
        "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
        "predicted_stock_price = pd.DataFrame(predicted_stock_price,columns=['adj_close'])\n",
        "predicted_stock_price.reset_index(inplace=True)\n",
        "predicted_stock_price = predicted_stock_price[['adj_close']]\n",
        "data_test = df[round(len(df)*factor_ratio):]\n",
        "data_test.reset_index(inplace=True)\n",
        "data_test = data_test[[\"Date\"]]\n",
        "\n",
        "final = pd.merge(data_test, predicted_stock_price, left_index=True, right_index=True)\n",
        "final.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>adj_close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-01-13</td>\n",
              "      <td>106.336983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-01-17</td>\n",
              "      <td>106.533737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-01-18</td>\n",
              "      <td>106.590538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-01-19</td>\n",
              "      <td>106.552277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-01-20</td>\n",
              "      <td>106.447083</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Date   adj_close\n",
              "0 2017-01-13  106.336983\n",
              "1 2017-01-17  106.533737\n",
              "2 2017-01-18  106.590538\n",
              "3 2017-01-19  106.552277\n",
              "4 2017-01-20  106.447083"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzElYx-nyh-x",
        "colab_type": "code",
        "outputId": "41fa62f3-111a-4c29-91b5-5ce6bfc263c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(real_stock_price, color = 'black', label = 'DIS Stock Price')\n",
        "plt.plot(predicted_stock_price['adj_close'], color = 'green', label = 'Predicted DIS Stock Price')\n",
        "plt.title('DIS Stock return Prediction')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('DIS Stock return')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMPl/Ntj4MFGHZB8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEO8DPvoGn2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('DIS_model.pickle','wb') as f:\n",
        "    pickle.dump(best_model,f)\n",
        "\n",
        "with open('DIS_model.pickle','rb') as f:\n",
        "    model = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lbroo87JvWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('DIS_model.pickle') "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}